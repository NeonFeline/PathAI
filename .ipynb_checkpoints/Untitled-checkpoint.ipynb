{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "edf73786-8025-42f8-aaa3-2431f7aad9bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maciej/miniconda3/envs/medi/lib/python3.12/site-packages/numba/np/ufunc/parallel.py:371: NumbaWarning: The TBB threading layer requires TBB version 2021 update 6 or later i.e., TBB_INTERFACE_VERSION >= 12060. Found TBB_INTERFACE_VERSION = 12050. The TBB threading layer is disabled.\n",
      "  warnings.warn(problem)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "from torchvision.io import read_image\n",
    "from torchvision.io import ImageReadMode\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoImageProcessor, AutoModel\n",
    "from PIL import Image\n",
    "import requests\n",
    "from torch.utils.data import DataLoader\n",
    "from medmnist import PathMNIST, INFO\n",
    "from QuantumSelfAttentionLayer import QuantumSelfAttentionLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98f11e53-51fa-4454-988d-19a1caf22e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "163879e8-37cb-49f9-b5d2-5900efb7a7ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "processor = AutoImageProcessor.from_pretrained('facebook/dinov2-base')\n",
    "model = AutoModel.from_pretrained('facebook/dinov2-base').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17c5f231-68b2-4039-845e-db964fc3a3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiseaseDataset(Dataset):\n",
    "    def __init__(self, newDatasetInput, newDatasetOutput):\n",
    "        self.newDatasetInput = torch.tensor(np.array(newDatasetInput)).to(device)\n",
    "        self.newDatasetOutput = torch.tensor(np.array(newDatasetOutput), dtype=torch.int64).to(device)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.newDatasetInput.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        embedding = self.newDatasetInput[idx]\n",
    "        label = self.newDatasetOutput[idx]\n",
    "\n",
    "        return embedding, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2c7c1a8-ec35-4cfc-943e-f4ca214a8c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download dataset info\n",
    "info = INFO['pathmnist']\n",
    "label_names = info['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3d733377-765c-43fc-b7e9-644a10bb2fe4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['adipose',\n",
       " 'background',\n",
       " 'debris',\n",
       " 'lymphocytes',\n",
       " 'mucus',\n",
       " 'smooth muscle',\n",
       " 'normal colon mucosa',\n",
       " 'cancer-associated stroma',\n",
       " 'colorectal adenocarcinoma epithelium']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e581b43-7c35-46da-ac32-61c679d5f9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_disease_dataset(save_dir):\n",
    "    input_path = os.path.join(save_dir, \"newDatasetInput.pt\")\n",
    "    output_path = os.path.join(save_dir, \"newDatasetOutput.pt\")\n",
    "\n",
    "    inputs = torch.load(input_path)\n",
    "    outputs = torch.load(output_path)\n",
    "    \n",
    "    return DiseaseDataset(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d0a7ed18-253d-4c7d-a087-83b4ea59c6f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_22680/2321754506.py:3: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments.\n",
      "  self.newDatasetInput = torch.tensor(np.array(newDatasetInput)).to(device)\n",
      "/tmp/ipykernel_22680/2321754506.py:4: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments.\n",
      "  self.newDatasetOutput = torch.tensor(np.array(newDatasetOutput), dtype=torch.int64).to(device)\n"
     ]
    }
   ],
   "source": [
    "train_dataset = load_disease_dataset(\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf878b95-5fcb-434a-9ea6-2673edef9ac9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_22680/2321754506.py:3: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments.\n",
      "  self.newDatasetInput = torch.tensor(np.array(newDatasetInput)).to(device)\n",
      "/tmp/ipykernel_22680/2321754506.py:4: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments.\n",
      "  self.newDatasetOutput = torch.tensor(np.array(newDatasetOutput), dtype=torch.int64).to(device)\n"
     ]
    }
   ],
   "source": [
    "test_dataset = load_disease_dataset(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "23206159-d079-41b8-a398-c3cbf2c33ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DiseaseClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear_gelu_stack1 = nn.Sequential(\n",
    "            nn.Linear(768, 512),\n",
    "            nn.GELU(),\n",
    "        )\n",
    "\n",
    "        self.quantum = nn.Sequential(\n",
    "            QuantumSelfAttentionLayer(512, 4, 5),\n",
    "            nn.Linear(5, 512),\n",
    "        )\n",
    "\n",
    "\n",
    "        self.linear_gelu_stack2 = nn.Sequential(\n",
    "            nn.Linear(512, 512),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(512, 9),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear_gelu_stack1(x)\n",
    "        q = self.quantum(x)\n",
    "        x = x + q\n",
    "        x = self.linear_gelu_stack2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7fa06a1b-31ca-4c4f-9f07-10a99d452c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiseaseClassifierFull(nn.Module):\n",
    "    def __init__(self, dino, processor, classifier):\n",
    "        super().__init__()\n",
    "        self.dino = dino\n",
    "        self.classifier = classifier\n",
    "        self.processor = processor\n",
    "\n",
    "    def forward(self, x):\n",
    "        inputs = self.processor(x, return_tensors=\"pt\").to(device)\n",
    "        outputs = self.dino(**inputs)\n",
    "        values = self.classifier(torch.mean(outputs.last_hidden_state, axis=1))\n",
    "        \n",
    "        return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "38eea1b8-e392-430d-9d8c-02b40aeb9707",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = DiseaseClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9d2566c4-cdfc-47bf-9f6c-0ca21f3c0843",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define optimizer and loss function\n",
    "optimizer = torch.optim.Adam(classifier.parameters(), lr=1e-4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# trainDataloader = DataLoader(trainingData, batch_size=batch_size, shuffle=True)\n",
    "# testDataloader = DataLoader(testData, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "\n",
    "epochs = 1000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "66ac7818-d0e0-464e-9974-0891e83ecc1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "310fa279-9c15-41b5-af37-ab6f5ea9475a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer, window_size=100):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    \n",
    "    # For tracking rolling averages\n",
    "    batch_losses = []\n",
    "    rolling_avg_losses = []\n",
    "    \n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Move data to device\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "        \n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "        \n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Track loss for rolling average\n",
    "        current_loss = loss.item()\n",
    "        batch_losses.append(current_loss)\n",
    "        \n",
    "        # Calculate rolling average\n",
    "        if len(batch_losses) > window_size:\n",
    "            batch_losses.pop(0)  # Remove oldest loss to maintain window size\n",
    "        \n",
    "        rolling_avg_loss = sum(batch_losses) / len(batch_losses)\n",
    "        rolling_avg_losses.append(rolling_avg_loss)\n",
    "        \n",
    "        if batch % 100 == 0:\n",
    "            current = batch * len(X)\n",
    "            print(f\"loss: {current_loss:>7f}, rolling avg loss: {rolling_avg_loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "    \n",
    "    # Return the rolling average losses for possible visualization\n",
    "    return rolling_avg_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aec1b876-fef7-4d1c-840a-e706d4121efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "580420cb-a574-4fd3-8f6c-f8d32a9ec29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4ce5bb94-e54e-4b5e-990c-8ca2903c8163",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "89de6f66-316e-4de8-9f5c-8ced2b6fb93a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maciej/Desktop/med/QuantumSelfAttentionLayer.py:58: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /pytorch/build/aten/src/ATen/core/TensorBody.h:489.)\n",
      "  if self.orig_query.grad is None:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.191085, rolling avg loss: 2.191085  [    0/89996]\n",
      "loss: 1.908982, rolling avg loss: 1.885128  [  100/89996]\n",
      "loss: 0.146316, rolling avg loss: 1.131842  [  200/89996]\n",
      "loss: 1.177288, rolling avg loss: 0.676339  [  300/89996]\n",
      "loss: 1.851651, rolling avg loss: 0.597197  [  400/89996]\n",
      "loss: 0.480742, rolling avg loss: 0.647838  [  500/89996]\n",
      "loss: 0.003085, rolling avg loss: 0.580665  [  600/89996]\n",
      "loss: 0.104599, rolling avg loss: 0.479732  [  700/89996]\n",
      "loss: 0.048798, rolling avg loss: 0.441239  [  800/89996]\n",
      "loss: 0.267393, rolling avg loss: 0.431533  [  900/89996]\n",
      "loss: 0.255576, rolling avg loss: 0.297778  [ 1000/89996]\n",
      "loss: 0.001311, rolling avg loss: 0.504680  [ 1100/89996]\n",
      "loss: 0.080491, rolling avg loss: 0.458503  [ 1200/89996]\n",
      "loss: 0.412693, rolling avg loss: 0.358777  [ 1300/89996]\n",
      "loss: 1.175157, rolling avg loss: 0.509278  [ 1400/89996]\n",
      "loss: 0.014059, rolling avg loss: 0.300433  [ 1500/89996]\n",
      "loss: 0.168135, rolling avg loss: 0.348651  [ 1600/89996]\n",
      "loss: 0.182561, rolling avg loss: 0.339328  [ 1700/89996]\n",
      "loss: 0.003300, rolling avg loss: 0.375410  [ 1800/89996]\n",
      "loss: 0.006165, rolling avg loss: 0.197519  [ 1900/89996]\n",
      "loss: 0.002116, rolling avg loss: 0.364477  [ 2000/89996]\n",
      "loss: 0.052610, rolling avg loss: 0.444205  [ 2100/89996]\n",
      "loss: 0.018317, rolling avg loss: 0.380120  [ 2200/89996]\n",
      "loss: 0.002028, rolling avg loss: 0.407773  [ 2300/89996]\n",
      "loss: 0.012889, rolling avg loss: 0.280539  [ 2400/89996]\n",
      "loss: 0.000100, rolling avg loss: 0.334834  [ 2500/89996]\n",
      "loss: 0.475962, rolling avg loss: 0.325783  [ 2600/89996]\n",
      "loss: 0.012882, rolling avg loss: 0.286447  [ 2700/89996]\n",
      "loss: 0.073862, rolling avg loss: 0.232190  [ 2800/89996]\n",
      "loss: 0.020509, rolling avg loss: 0.382999  [ 2900/89996]\n",
      "loss: 0.008988, rolling avg loss: 0.422935  [ 3000/89996]\n",
      "loss: 1.689866, rolling avg loss: 0.343426  [ 3100/89996]\n",
      "loss: 0.004538, rolling avg loss: 0.426496  [ 3200/89996]\n",
      "loss: 0.004461, rolling avg loss: 0.186520  [ 3300/89996]\n",
      "loss: 0.010719, rolling avg loss: 0.255103  [ 3400/89996]\n",
      "loss: 0.700001, rolling avg loss: 0.273158  [ 3500/89996]\n",
      "loss: 0.037924, rolling avg loss: 0.330555  [ 3600/89996]\n",
      "loss: 0.005917, rolling avg loss: 0.197519  [ 3700/89996]\n",
      "loss: 0.032306, rolling avg loss: 0.357133  [ 3800/89996]\n",
      "loss: 0.013692, rolling avg loss: 0.283518  [ 3900/89996]\n",
      "loss: 0.000557, rolling avg loss: 0.235891  [ 4000/89996]\n",
      "loss: 0.108293, rolling avg loss: 0.351897  [ 4100/89996]\n",
      "loss: 0.010461, rolling avg loss: 0.405685  [ 4200/89996]\n",
      "loss: 2.220680, rolling avg loss: 0.305172  [ 4300/89996]\n",
      "loss: 0.008115, rolling avg loss: 0.260494  [ 4400/89996]\n",
      "loss: 7.735987, rolling avg loss: 0.419049  [ 4500/89996]\n",
      "loss: 0.005675, rolling avg loss: 0.328908  [ 4600/89996]\n",
      "loss: 0.000594, rolling avg loss: 0.259127  [ 4700/89996]\n",
      "loss: 0.063867, rolling avg loss: 0.265565  [ 4800/89996]\n",
      "loss: 0.005265, rolling avg loss: 0.230342  [ 4900/89996]\n",
      "loss: 0.000098, rolling avg loss: 0.251174  [ 5000/89996]\n",
      "loss: 0.333279, rolling avg loss: 0.230037  [ 5100/89996]\n",
      "loss: 0.311997, rolling avg loss: 0.203969  [ 5200/89996]\n",
      "loss: 0.000522, rolling avg loss: 0.281735  [ 5300/89996]\n",
      "loss: 0.004351, rolling avg loss: 0.231232  [ 5400/89996]\n",
      "loss: 0.000050, rolling avg loss: 0.284940  [ 5500/89996]\n",
      "loss: 0.119676, rolling avg loss: 0.277434  [ 5600/89996]\n",
      "loss: 0.002344, rolling avg loss: 0.252286  [ 5700/89996]\n",
      "loss: 0.005710, rolling avg loss: 0.322855  [ 5800/89996]\n",
      "loss: 0.032345, rolling avg loss: 0.207344  [ 5900/89996]\n",
      "loss: 0.085685, rolling avg loss: 0.351190  [ 6000/89996]\n",
      "loss: 0.044803, rolling avg loss: 0.291420  [ 6100/89996]\n",
      "loss: 0.001024, rolling avg loss: 0.331093  [ 6200/89996]\n",
      "loss: 0.002609, rolling avg loss: 0.192290  [ 6300/89996]\n",
      "loss: 0.306785, rolling avg loss: 0.306672  [ 6400/89996]\n",
      "loss: 0.000078, rolling avg loss: 0.287864  [ 6500/89996]\n",
      "loss: 0.000830, rolling avg loss: 0.292785  [ 6600/89996]\n",
      "loss: 0.010651, rolling avg loss: 0.185712  [ 6700/89996]\n",
      "loss: 0.008464, rolling avg loss: 0.246057  [ 6800/89996]\n",
      "loss: 0.007627, rolling avg loss: 0.268713  [ 6900/89996]\n",
      "loss: 0.036896, rolling avg loss: 0.240012  [ 7000/89996]\n",
      "loss: 0.009249, rolling avg loss: 0.265342  [ 7100/89996]\n",
      "loss: 0.451475, rolling avg loss: 0.364738  [ 7200/89996]\n",
      "loss: 0.000089, rolling avg loss: 0.167916  [ 7300/89996]\n",
      "loss: 0.000009, rolling avg loss: 0.132907  [ 7400/89996]\n",
      "loss: 0.001182, rolling avg loss: 0.119405  [ 7500/89996]\n",
      "loss: 0.000753, rolling avg loss: 0.190988  [ 7600/89996]\n",
      "loss: 0.000016, rolling avg loss: 0.102897  [ 7700/89996]\n",
      "loss: 3.185083, rolling avg loss: 0.306007  [ 7800/89996]\n",
      "loss: 0.016691, rolling avg loss: 0.230215  [ 7900/89996]\n",
      "loss: 0.006499, rolling avg loss: 0.195389  [ 8000/89996]\n",
      "loss: 0.000706, rolling avg loss: 0.326618  [ 8100/89996]\n",
      "loss: 0.544722, rolling avg loss: 0.331460  [ 8200/89996]\n",
      "loss: 0.154164, rolling avg loss: 0.362790  [ 8300/89996]\n",
      "loss: 0.028730, rolling avg loss: 0.210857  [ 8400/89996]\n",
      "loss: 0.017682, rolling avg loss: 0.136290  [ 8500/89996]\n",
      "loss: 0.001278, rolling avg loss: 0.213209  [ 8600/89996]\n",
      "loss: 0.000302, rolling avg loss: 0.234383  [ 8700/89996]\n",
      "loss: 0.021443, rolling avg loss: 0.276373  [ 8800/89996]\n",
      "loss: 0.118328, rolling avg loss: 0.168552  [ 8900/89996]\n",
      "loss: 0.000408, rolling avg loss: 0.193873  [ 9000/89996]\n",
      "loss: 0.000028, rolling avg loss: 0.155932  [ 9100/89996]\n",
      "loss: 0.209114, rolling avg loss: 0.239121  [ 9200/89996]\n",
      "loss: 0.106905, rolling avg loss: 0.162719  [ 9300/89996]\n",
      "loss: 2.266941, rolling avg loss: 0.223760  [ 9400/89996]\n",
      "loss: 0.022455, rolling avg loss: 0.123234  [ 9500/89996]\n",
      "loss: 0.013594, rolling avg loss: 0.165142  [ 9600/89996]\n",
      "loss: 0.170111, rolling avg loss: 0.259441  [ 9700/89996]\n",
      "loss: 0.652298, rolling avg loss: 0.121479  [ 9800/89996]\n",
      "loss: 0.009444, rolling avg loss: 0.185131  [ 9900/89996]\n",
      "loss: 0.008005, rolling avg loss: 0.390414  [10000/89996]\n",
      "loss: 2.864955, rolling avg loss: 0.344757  [10100/89996]\n",
      "loss: 0.032508, rolling avg loss: 0.149314  [10200/89996]\n",
      "loss: 0.001159, rolling avg loss: 0.203718  [10300/89996]\n",
      "loss: 0.288982, rolling avg loss: 0.344716  [10400/89996]\n",
      "loss: 0.027581, rolling avg loss: 0.131404  [10500/89996]\n",
      "loss: 0.000326, rolling avg loss: 0.265530  [10600/89996]\n",
      "loss: 0.177362, rolling avg loss: 0.154200  [10700/89996]\n",
      "loss: 0.008321, rolling avg loss: 0.206111  [10800/89996]\n",
      "loss: 0.023344, rolling avg loss: 0.146135  [10900/89996]\n",
      "loss: 0.000741, rolling avg loss: 0.302388  [11000/89996]\n",
      "loss: 0.001175, rolling avg loss: 0.204222  [11100/89996]\n",
      "loss: 0.077885, rolling avg loss: 0.193956  [11200/89996]\n",
      "loss: 0.000372, rolling avg loss: 0.132346  [11300/89996]\n",
      "loss: 0.000821, rolling avg loss: 0.142938  [11400/89996]\n",
      "loss: 0.000097, rolling avg loss: 0.191428  [11500/89996]\n",
      "loss: 0.004587, rolling avg loss: 0.452187  [11600/89996]\n",
      "loss: 0.124908, rolling avg loss: 0.297027  [11700/89996]\n",
      "loss: 0.000313, rolling avg loss: 0.206003  [11800/89996]\n",
      "loss: 0.000702, rolling avg loss: 0.193955  [11900/89996]\n",
      "loss: 1.229855, rolling avg loss: 0.208372  [12000/89996]\n",
      "loss: 0.674506, rolling avg loss: 0.384912  [12100/89996]\n",
      "loss: 0.000020, rolling avg loss: 0.273780  [12200/89996]\n",
      "loss: 0.032632, rolling avg loss: 0.165940  [12300/89996]\n",
      "loss: 0.000303, rolling avg loss: 0.224244  [12400/89996]\n",
      "loss: 0.015581, rolling avg loss: 0.170651  [12500/89996]\n",
      "loss: 0.000662, rolling avg loss: 0.204344  [12600/89996]\n",
      "loss: 0.000529, rolling avg loss: 0.190293  [12700/89996]\n",
      "loss: 0.006123, rolling avg loss: 0.107558  [12800/89996]\n",
      "loss: 0.091330, rolling avg loss: 0.298204  [12900/89996]\n",
      "loss: 0.097156, rolling avg loss: 0.239261  [13000/89996]\n",
      "loss: 1.668375, rolling avg loss: 0.267309  [13100/89996]\n",
      "loss: 3.550030, rolling avg loss: 0.150831  [13200/89996]\n",
      "loss: 0.011824, rolling avg loss: 0.208940  [13300/89996]\n",
      "loss: 0.000810, rolling avg loss: 0.184696  [13400/89996]\n",
      "loss: 0.002008, rolling avg loss: 0.210360  [13500/89996]\n",
      "loss: 0.524792, rolling avg loss: 0.272284  [13600/89996]\n",
      "loss: 0.544228, rolling avg loss: 0.324854  [13700/89996]\n",
      "loss: 0.008844, rolling avg loss: 0.190010  [13800/89996]\n",
      "loss: 0.014099, rolling avg loss: 0.235371  [13900/89996]\n",
      "loss: 0.014954, rolling avg loss: 0.267407  [14000/89996]\n",
      "loss: 0.014491, rolling avg loss: 0.216274  [14100/89996]\n",
      "loss: 0.039848, rolling avg loss: 0.123033  [14200/89996]\n",
      "loss: 0.244015, rolling avg loss: 0.223928  [14300/89996]\n",
      "loss: 0.276553, rolling avg loss: 0.485063  [14400/89996]\n",
      "loss: 0.070732, rolling avg loss: 0.105018  [14500/89996]\n",
      "loss: 0.000263, rolling avg loss: 0.152638  [14600/89996]\n",
      "loss: 0.096229, rolling avg loss: 0.096308  [14700/89996]\n",
      "loss: 0.015319, rolling avg loss: 0.221833  [14800/89996]\n",
      "loss: 2.528787, rolling avg loss: 0.167112  [14900/89996]\n",
      "loss: 1.079685, rolling avg loss: 0.140484  [15000/89996]\n",
      "loss: 0.012482, rolling avg loss: 0.336657  [15100/89996]\n",
      "loss: 0.047733, rolling avg loss: 0.150694  [15200/89996]\n",
      "loss: 0.012205, rolling avg loss: 0.090713  [15300/89996]\n",
      "loss: 0.006882, rolling avg loss: 0.160748  [15400/89996]\n",
      "loss: 0.000123, rolling avg loss: 0.070706  [15500/89996]\n",
      "loss: 0.005436, rolling avg loss: 0.110668  [15600/89996]\n",
      "loss: 0.000020, rolling avg loss: 0.092569  [15700/89996]\n",
      "loss: 0.198501, rolling avg loss: 0.396863  [15800/89996]\n",
      "loss: 0.000728, rolling avg loss: 0.227953  [15900/89996]\n",
      "loss: 0.000002, rolling avg loss: 0.140419  [16000/89996]\n",
      "loss: 0.000568, rolling avg loss: 0.169608  [16100/89996]\n",
      "loss: 0.000244, rolling avg loss: 0.154704  [16200/89996]\n",
      "loss: 0.013073, rolling avg loss: 0.311017  [16300/89996]\n",
      "loss: 0.010133, rolling avg loss: 0.140811  [16400/89996]\n",
      "loss: 0.003786, rolling avg loss: 0.051765  [16500/89996]\n",
      "loss: 0.000229, rolling avg loss: 0.167544  [16600/89996]\n",
      "loss: 0.002691, rolling avg loss: 0.207229  [16700/89996]\n",
      "loss: 0.000086, rolling avg loss: 0.177150  [16800/89996]\n",
      "loss: 0.008621, rolling avg loss: 0.249708  [16900/89996]\n",
      "loss: 0.002439, rolling avg loss: 0.190581  [17000/89996]\n",
      "loss: 0.003142, rolling avg loss: 0.145506  [17100/89996]\n",
      "loss: 0.019492, rolling avg loss: 0.331963  [17200/89996]\n",
      "loss: 0.000091, rolling avg loss: 0.118268  [17300/89996]\n",
      "loss: 0.002204, rolling avg loss: 0.268139  [17400/89996]\n",
      "loss: 1.845571, rolling avg loss: 0.221927  [17500/89996]\n",
      "loss: 0.261816, rolling avg loss: 0.179469  [17600/89996]\n",
      "loss: 0.000224, rolling avg loss: 0.094658  [17700/89996]\n",
      "loss: 0.003434, rolling avg loss: 0.163511  [17800/89996]\n",
      "loss: 0.003284, rolling avg loss: 0.235303  [17900/89996]\n",
      "loss: 0.054889, rolling avg loss: 0.120271  [18000/89996]\n",
      "loss: 0.000086, rolling avg loss: 0.111862  [18100/89996]\n",
      "loss: 0.050047, rolling avg loss: 0.277336  [18200/89996]\n",
      "loss: 0.000064, rolling avg loss: 0.272143  [18300/89996]\n",
      "loss: 0.000019, rolling avg loss: 0.181251  [18400/89996]\n",
      "loss: 0.023594, rolling avg loss: 0.315620  [18500/89996]\n",
      "loss: 0.007230, rolling avg loss: 0.145017  [18600/89996]\n",
      "loss: 0.000947, rolling avg loss: 0.303145  [18700/89996]\n",
      "loss: 0.003228, rolling avg loss: 0.236742  [18800/89996]\n",
      "loss: 0.000045, rolling avg loss: 0.139981  [18900/89996]\n",
      "loss: 0.000213, rolling avg loss: 0.151865  [19000/89996]\n",
      "loss: 0.033674, rolling avg loss: 0.070328  [19100/89996]\n",
      "loss: 0.002787, rolling avg loss: 0.035619  [19200/89996]\n",
      "loss: 0.000274, rolling avg loss: 0.111179  [19300/89996]\n",
      "loss: 0.008603, rolling avg loss: 0.161242  [19400/89996]\n",
      "loss: 0.043284, rolling avg loss: 0.205733  [19500/89996]\n",
      "loss: 0.106986, rolling avg loss: 0.157892  [19600/89996]\n",
      "loss: 0.000394, rolling avg loss: 0.216506  [19700/89996]\n",
      "loss: 0.000107, rolling avg loss: 0.182545  [19800/89996]\n",
      "loss: 0.000311, rolling avg loss: 0.097808  [19900/89996]\n",
      "loss: 0.010525, rolling avg loss: 0.208685  [20000/89996]\n",
      "loss: 0.000195, rolling avg loss: 0.057395  [20100/89996]\n",
      "loss: 0.000173, rolling avg loss: 0.081203  [20200/89996]\n",
      "loss: 0.000971, rolling avg loss: 0.217144  [20300/89996]\n",
      "loss: 0.000031, rolling avg loss: 0.133018  [20400/89996]\n",
      "loss: 0.000311, rolling avg loss: 0.179583  [20500/89996]\n",
      "loss: 0.002652, rolling avg loss: 0.125537  [20600/89996]\n",
      "loss: 0.001504, rolling avg loss: 0.249869  [20700/89996]\n",
      "loss: 0.000015, rolling avg loss: 0.123497  [20800/89996]\n",
      "loss: 0.000497, rolling avg loss: 0.205731  [20900/89996]\n",
      "loss: 0.282767, rolling avg loss: 0.292852  [21000/89996]\n",
      "loss: 0.001262, rolling avg loss: 0.180263  [21100/89996]\n",
      "loss: 0.000003, rolling avg loss: 0.201932  [21200/89996]\n",
      "loss: 0.001805, rolling avg loss: 0.216964  [21300/89996]\n",
      "loss: 0.000001, rolling avg loss: 0.116968  [21400/89996]\n",
      "loss: 0.005474, rolling avg loss: 0.256344  [21500/89996]\n",
      "loss: 0.000001, rolling avg loss: 0.200100  [21600/89996]\n",
      "loss: 0.004288, rolling avg loss: 0.121740  [21700/89996]\n",
      "loss: 0.000005, rolling avg loss: 0.150195  [21800/89996]\n",
      "loss: 0.002071, rolling avg loss: 0.094053  [21900/89996]\n",
      "loss: 0.000000, rolling avg loss: 0.234214  [22000/89996]\n",
      "loss: 0.000079, rolling avg loss: 0.288097  [22100/89996]\n",
      "loss: 0.042715, rolling avg loss: 0.055603  [22200/89996]\n",
      "loss: 0.000389, rolling avg loss: 0.189989  [22300/89996]\n",
      "loss: 0.000103, rolling avg loss: 0.145809  [22400/89996]\n",
      "loss: 0.000084, rolling avg loss: 0.094177  [22500/89996]\n",
      "loss: 0.000014, rolling avg loss: 0.245907  [22600/89996]\n",
      "loss: 0.000862, rolling avg loss: 0.208907  [22700/89996]\n",
      "loss: 0.000083, rolling avg loss: 0.176781  [22800/89996]\n",
      "loss: 0.004687, rolling avg loss: 0.267817  [22900/89996]\n",
      "loss: 0.002861, rolling avg loss: 0.149420  [23000/89996]\n",
      "loss: 0.000025, rolling avg loss: 0.179262  [23100/89996]\n",
      "loss: 0.021155, rolling avg loss: 0.077910  [23200/89996]\n",
      "loss: 0.005899, rolling avg loss: 0.198945  [23300/89996]\n",
      "loss: 0.022095, rolling avg loss: 0.086463  [23400/89996]\n",
      "loss: 0.000001, rolling avg loss: 0.078981  [23500/89996]\n",
      "loss: 0.000217, rolling avg loss: 0.133094  [23600/89996]\n",
      "loss: 0.000038, rolling avg loss: 0.276431  [23700/89996]\n",
      "loss: 0.000481, rolling avg loss: 0.193741  [23800/89996]\n",
      "loss: 0.031749, rolling avg loss: 0.114135  [23900/89996]\n",
      "loss: 0.000000, rolling avg loss: 0.147729  [24000/89996]\n",
      "loss: 0.000037, rolling avg loss: 0.147121  [24100/89996]\n",
      "loss: 0.000005, rolling avg loss: 0.091878  [24200/89996]\n",
      "loss: 0.011783, rolling avg loss: 0.272030  [24300/89996]\n",
      "loss: 0.000115, rolling avg loss: 0.162801  [24400/89996]\n",
      "loss: 0.000017, rolling avg loss: 0.127656  [24500/89996]\n",
      "loss: 0.000006, rolling avg loss: 0.175643  [24600/89996]\n",
      "loss: 0.028193, rolling avg loss: 0.170990  [24700/89996]\n",
      "loss: 0.000152, rolling avg loss: 0.259664  [24800/89996]\n",
      "loss: 0.000525, rolling avg loss: 0.179914  [24900/89996]\n",
      "loss: 3.672639, rolling avg loss: 0.167074  [25000/89996]\n",
      "loss: 0.033100, rolling avg loss: 0.181998  [25100/89996]\n",
      "loss: 0.985240, rolling avg loss: 0.380338  [25200/89996]\n",
      "loss: 0.038121, rolling avg loss: 0.114151  [25300/89996]\n",
      "loss: 2.729378, rolling avg loss: 0.118331  [25400/89996]\n",
      "loss: 0.000007, rolling avg loss: 0.303328  [25500/89996]\n",
      "loss: 0.000048, rolling avg loss: 0.223422  [25600/89996]\n",
      "loss: 0.000524, rolling avg loss: 0.138390  [25700/89996]\n",
      "loss: 0.780436, rolling avg loss: 0.146925  [25800/89996]\n",
      "loss: 0.020268, rolling avg loss: 0.084725  [25900/89996]\n",
      "loss: 0.000027, rolling avg loss: 0.243852  [26000/89996]\n",
      "loss: 0.001632, rolling avg loss: 0.045455  [26100/89996]\n",
      "loss: 0.000008, rolling avg loss: 0.280549  [26200/89996]\n",
      "loss: 0.006653, rolling avg loss: 0.233355  [26300/89996]\n",
      "loss: 0.003519, rolling avg loss: 0.146645  [26400/89996]\n",
      "loss: 0.013492, rolling avg loss: 0.334959  [26500/89996]\n",
      "loss: 0.020058, rolling avg loss: 0.128605  [26600/89996]\n",
      "loss: 0.004121, rolling avg loss: 0.103032  [26700/89996]\n",
      "loss: 0.000891, rolling avg loss: 0.230744  [26800/89996]\n",
      "loss: 0.075981, rolling avg loss: 0.357056  [26900/89996]\n",
      "loss: 0.000016, rolling avg loss: 0.186725  [27000/89996]\n",
      "loss: 0.006589, rolling avg loss: 0.106118  [27100/89996]\n",
      "loss: 0.000440, rolling avg loss: 0.110499  [27200/89996]\n",
      "loss: 0.010249, rolling avg loss: 0.157607  [27300/89996]\n",
      "loss: 0.000003, rolling avg loss: 0.252198  [27400/89996]\n",
      "loss: 0.003848, rolling avg loss: 0.119020  [27500/89996]\n",
      "loss: 0.001852, rolling avg loss: 0.255656  [27600/89996]\n",
      "loss: 0.000631, rolling avg loss: 0.187237  [27700/89996]\n",
      "loss: 0.001119, rolling avg loss: 0.226598  [27800/89996]\n",
      "loss: 0.000809, rolling avg loss: 0.143199  [27900/89996]\n",
      "loss: 0.002170, rolling avg loss: 0.088842  [28000/89996]\n",
      "loss: 0.007577, rolling avg loss: 0.240699  [28100/89996]\n",
      "loss: 0.000000, rolling avg loss: 0.194826  [28200/89996]\n",
      "loss: 0.000041, rolling avg loss: 0.185969  [28300/89996]\n",
      "loss: 0.000455, rolling avg loss: 0.087835  [28400/89996]\n",
      "loss: 0.048798, rolling avg loss: 0.195700  [28500/89996]\n",
      "loss: 0.004577, rolling avg loss: 0.113296  [28600/89996]\n",
      "loss: 0.405795, rolling avg loss: 0.099806  [28700/89996]\n",
      "loss: 0.053750, rolling avg loss: 0.038673  [28800/89996]\n",
      "loss: 0.014662, rolling avg loss: 0.310819  [28900/89996]\n",
      "loss: 0.000007, rolling avg loss: 0.276443  [29000/89996]\n",
      "loss: 1.723428, rolling avg loss: 0.177249  [29100/89996]\n",
      "loss: 0.063612, rolling avg loss: 0.397343  [29200/89996]\n",
      "loss: 0.004291, rolling avg loss: 0.064993  [29300/89996]\n",
      "loss: 0.016025, rolling avg loss: 0.217567  [29400/89996]\n",
      "loss: 0.000000, rolling avg loss: 0.221648  [29500/89996]\n",
      "loss: 0.000000, rolling avg loss: 0.223113  [29600/89996]\n",
      "loss: 0.007764, rolling avg loss: 0.124802  [29700/89996]\n",
      "loss: 0.002104, rolling avg loss: 0.158772  [29800/89996]\n",
      "loss: 0.003267, rolling avg loss: 0.144005  [29900/89996]\n",
      "loss: 0.009170, rolling avg loss: 0.169579  [30000/89996]\n",
      "loss: 0.000000, rolling avg loss: 0.121663  [30100/89996]\n",
      "loss: 0.232712, rolling avg loss: 0.218482  [30200/89996]\n",
      "loss: 0.000008, rolling avg loss: 0.115896  [30300/89996]\n",
      "loss: 0.010897, rolling avg loss: 0.124384  [30400/89996]\n",
      "loss: 0.027764, rolling avg loss: 0.191885  [30500/89996]\n",
      "loss: 0.000046, rolling avg loss: 0.163264  [30600/89996]\n",
      "loss: 0.000001, rolling avg loss: 0.176040  [30700/89996]\n",
      "loss: 0.000639, rolling avg loss: 0.114823  [30800/89996]\n",
      "loss: 0.000043, rolling avg loss: 0.145759  [30900/89996]\n",
      "loss: 0.052377, rolling avg loss: 0.146832  [31000/89996]\n",
      "loss: 0.000092, rolling avg loss: 0.038499  [31100/89996]\n",
      "loss: 0.001366, rolling avg loss: 0.242681  [31200/89996]\n",
      "loss: 0.000047, rolling avg loss: 0.144959  [31300/89996]\n",
      "loss: 0.000240, rolling avg loss: 0.288056  [31400/89996]\n",
      "loss: 0.000000, rolling avg loss: 0.152699  [31500/89996]\n",
      "loss: 0.000000, rolling avg loss: 0.152187  [31600/89996]\n",
      "loss: 0.001613, rolling avg loss: 0.166211  [31700/89996]\n",
      "loss: 0.038375, rolling avg loss: 0.209672  [31800/89996]\n",
      "loss: 0.000424, rolling avg loss: 0.145598  [31900/89996]\n",
      "loss: 0.002191, rolling avg loss: 0.175359  [32000/89996]\n",
      "loss: 0.000389, rolling avg loss: 0.145672  [32100/89996]\n",
      "loss: 0.012356, rolling avg loss: 0.136084  [32200/89996]\n",
      "loss: 0.000040, rolling avg loss: 0.146606  [32300/89996]\n",
      "loss: 0.000000, rolling avg loss: 0.141733  [32400/89996]\n",
      "loss: 0.257474, rolling avg loss: 0.164700  [32500/89996]\n",
      "loss: 0.070048, rolling avg loss: 0.127627  [32600/89996]\n",
      "loss: 0.719569, rolling avg loss: 0.170452  [32700/89996]\n",
      "loss: 0.000069, rolling avg loss: 0.130036  [32800/89996]\n",
      "loss: 0.013629, rolling avg loss: 0.143687  [32900/89996]\n",
      "loss: 0.000706, rolling avg loss: 0.068412  [33000/89996]\n",
      "loss: 0.000599, rolling avg loss: 0.128445  [33100/89996]\n",
      "loss: 0.000000, rolling avg loss: 0.180559  [33200/89996]\n",
      "loss: 0.012307, rolling avg loss: 0.119814  [33300/89996]\n",
      "loss: 0.041638, rolling avg loss: 0.294059  [33400/89996]\n",
      "loss: 0.000000, rolling avg loss: 0.087980  [33500/89996]\n",
      "loss: 0.069686, rolling avg loss: 0.185022  [33600/89996]\n",
      "loss: 0.001695, rolling avg loss: 0.086436  [33700/89996]\n",
      "loss: 0.000064, rolling avg loss: 0.149157  [33800/89996]\n",
      "loss: 0.000000, rolling avg loss: 0.063846  [33900/89996]\n",
      "loss: 0.000000, rolling avg loss: 0.129089  [34000/89996]\n",
      "loss: 0.000474, rolling avg loss: 0.070058  [34100/89996]\n",
      "loss: 0.000000, rolling avg loss: 0.131346  [34200/89996]\n",
      "loss: 0.002219, rolling avg loss: 0.120301  [34300/89996]\n",
      "loss: 1.488421, rolling avg loss: 0.097220  [34400/89996]\n",
      "loss: 0.000000, rolling avg loss: 0.303438  [34500/89996]\n",
      "loss: 0.034434, rolling avg loss: 0.135373  [34600/89996]\n",
      "loss: 0.000000, rolling avg loss: 0.072601  [34700/89996]\n",
      "loss: 0.000076, rolling avg loss: 0.106987  [34800/89996]\n",
      "loss: 0.001509, rolling avg loss: 0.194450  [34900/89996]\n",
      "loss: 0.001643, rolling avg loss: 0.058209  [35000/89996]\n",
      "loss: 2.084062, rolling avg loss: 0.235657  [35100/89996]\n",
      "loss: 0.069586, rolling avg loss: 0.160106  [35200/89996]\n",
      "loss: 0.173405, rolling avg loss: 0.220029  [35300/89996]\n",
      "loss: 0.000794, rolling avg loss: 0.211449  [35400/89996]\n",
      "loss: 0.000000, rolling avg loss: 0.257789  [35500/89996]\n",
      "loss: 0.044490, rolling avg loss: 0.169256  [35600/89996]\n",
      "loss: 0.000001, rolling avg loss: 0.179367  [35700/89996]\n",
      "loss: 0.104396, rolling avg loss: 0.174493  [35800/89996]\n",
      "loss: 0.000008, rolling avg loss: 0.252428  [35900/89996]\n",
      "loss: 0.216931, rolling avg loss: 0.053911  [36000/89996]\n",
      "loss: 0.000064, rolling avg loss: 0.207182  [36100/89996]\n",
      "loss: 0.000001, rolling avg loss: 0.110868  [36200/89996]\n",
      "loss: 0.009330, rolling avg loss: 0.296341  [36300/89996]\n",
      "loss: 0.002843, rolling avg loss: 0.152680  [36400/89996]\n",
      "loss: 0.000105, rolling avg loss: 0.069847  [36500/89996]\n",
      "loss: 0.000010, rolling avg loss: 0.125295  [36600/89996]\n",
      "loss: 2.960934, rolling avg loss: 0.160933  [36700/89996]\n",
      "loss: 0.000047, rolling avg loss: 0.164959  [36800/89996]\n",
      "loss: 0.166349, rolling avg loss: 0.307668  [36900/89996]\n",
      "loss: 0.001504, rolling avg loss: 0.216242  [37000/89996]\n",
      "loss: 0.036627, rolling avg loss: 0.213454  [37100/89996]\n",
      "loss: 0.002109, rolling avg loss: 0.172826  [37200/89996]\n",
      "loss: 0.004178, rolling avg loss: 0.051263  [37300/89996]\n",
      "loss: 0.000264, rolling avg loss: 0.226647  [37400/89996]\n",
      "loss: 0.000207, rolling avg loss: 0.097107  [37500/89996]\n",
      "loss: 0.002739, rolling avg loss: 0.150957  [37600/89996]\n",
      "loss: 0.000707, rolling avg loss: 0.040847  [37700/89996]\n",
      "loss: 0.005127, rolling avg loss: 0.082155  [37800/89996]\n",
      "loss: 0.000716, rolling avg loss: 0.120034  [37900/89996]\n",
      "loss: 0.000318, rolling avg loss: 0.158645  [38000/89996]\n",
      "loss: 0.000003, rolling avg loss: 0.082314  [38100/89996]\n",
      "loss: 0.375591, rolling avg loss: 0.263844  [38200/89996]\n",
      "loss: 0.000002, rolling avg loss: 0.205550  [38300/89996]\n",
      "loss: 0.000029, rolling avg loss: 0.116496  [38400/89996]\n",
      "loss: 0.000001, rolling avg loss: 0.166012  [38500/89996]\n",
      "loss: 0.000000, rolling avg loss: 0.124272  [38600/89996]\n",
      "loss: 0.164359, rolling avg loss: 0.226307  [38700/89996]\n",
      "loss: 0.025615, rolling avg loss: 0.108916  [38800/89996]\n",
      "loss: 0.000001, rolling avg loss: 0.100897  [38900/89996]\n",
      "loss: 0.000006, rolling avg loss: 0.054921  [39000/89996]\n",
      "loss: 0.000000, rolling avg loss: 0.149717  [39100/89996]\n",
      "loss: 0.000034, rolling avg loss: 0.208698  [39200/89996]\n",
      "loss: 0.082160, rolling avg loss: 0.155024  [39300/89996]\n",
      "loss: 0.000601, rolling avg loss: 0.114228  [39400/89996]\n",
      "loss: 0.000544, rolling avg loss: 0.089474  [39500/89996]\n",
      "loss: 0.149425, rolling avg loss: 0.290226  [39600/89996]\n",
      "loss: 0.001555, rolling avg loss: 0.126572  [39700/89996]\n",
      "loss: 2.357296, rolling avg loss: 0.170306  [39800/89996]\n",
      "loss: 0.002009, rolling avg loss: 0.055842  [39900/89996]\n",
      "loss: 0.016041, rolling avg loss: 0.208052  [40000/89996]\n",
      "loss: 0.000002, rolling avg loss: 0.081856  [40100/89996]\n",
      "loss: 0.102529, rolling avg loss: 0.203742  [40200/89996]\n",
      "loss: 0.164641, rolling avg loss: 0.145155  [40300/89996]\n",
      "loss: 0.000953, rolling avg loss: 0.064687  [40400/89996]\n",
      "loss: 0.000048, rolling avg loss: 0.184855  [40500/89996]\n",
      "loss: 0.000461, rolling avg loss: 0.032060  [40600/89996]\n",
      "loss: 3.320722, rolling avg loss: 0.214197  [40700/89996]\n",
      "loss: 1.550859, rolling avg loss: 0.354088  [40800/89996]\n",
      "loss: 0.000357, rolling avg loss: 0.189723  [40900/89996]\n",
      "loss: 0.011263, rolling avg loss: 0.161810  [41000/89996]\n",
      "loss: 0.151934, rolling avg loss: 0.168894  [41100/89996]\n",
      "loss: 0.000059, rolling avg loss: 0.128950  [41200/89996]\n",
      "loss: 0.000009, rolling avg loss: 0.211503  [41300/89996]\n",
      "loss: 0.000250, rolling avg loss: 0.097567  [41400/89996]\n",
      "loss: 0.006669, rolling avg loss: 0.190692  [41500/89996]\n",
      "loss: 0.001237, rolling avg loss: 0.102367  [41600/89996]\n",
      "loss: 0.021905, rolling avg loss: 0.188065  [41700/89996]\n",
      "loss: 0.000948, rolling avg loss: 0.174324  [41800/89996]\n",
      "loss: 0.000000, rolling avg loss: 0.120033  [41900/89996]\n",
      "loss: 0.000197, rolling avg loss: 0.028787  [42000/89996]\n",
      "loss: 0.072264, rolling avg loss: 0.125784  [42100/89996]\n",
      "loss: 0.001050, rolling avg loss: 0.113674  [42200/89996]\n",
      "loss: 0.253908, rolling avg loss: 0.234009  [42300/89996]\n",
      "loss: 0.497524, rolling avg loss: 0.208957  [42400/89996]\n",
      "loss: 0.666193, rolling avg loss: 0.250009  [42500/89996]\n",
      "loss: 0.023092, rolling avg loss: 0.122399  [42600/89996]\n",
      "loss: 0.000280, rolling avg loss: 0.071083  [42700/89996]\n",
      "loss: 0.000000, rolling avg loss: 0.153506  [42800/89996]\n",
      "loss: 0.000000, rolling avg loss: 0.075204  [42900/89996]\n",
      "loss: 0.000000, rolling avg loss: 0.252684  [43000/89996]\n",
      "loss: 0.000000, rolling avg loss: 0.230248  [43100/89996]\n",
      "loss: 0.000002, rolling avg loss: 0.035863  [43200/89996]\n",
      "loss: 0.013129, rolling avg loss: 0.198283  [43300/89996]\n",
      "loss: 0.017508, rolling avg loss: 0.168329  [43400/89996]\n",
      "loss: 0.015256, rolling avg loss: 0.178863  [43500/89996]\n",
      "loss: 0.002640, rolling avg loss: 0.257159  [43600/89996]\n",
      "loss: 0.000055, rolling avg loss: 0.148661  [43700/89996]\n",
      "loss: 0.009860, rolling avg loss: 0.172175  [43800/89996]\n",
      "loss: 0.036262, rolling avg loss: 0.206863  [43900/89996]\n",
      "loss: 0.023876, rolling avg loss: 0.130802  [44000/89996]\n",
      "loss: 0.000206, rolling avg loss: 0.257423  [44100/89996]\n",
      "loss: 0.004829, rolling avg loss: 0.113356  [44200/89996]\n",
      "loss: 0.000014, rolling avg loss: 0.159193  [44300/89996]\n",
      "loss: 0.005115, rolling avg loss: 0.082460  [44400/89996]\n",
      "loss: 0.000097, rolling avg loss: 0.199786  [44500/89996]\n",
      "loss: 0.018531, rolling avg loss: 0.162185  [44600/89996]\n",
      "loss: 0.000000, rolling avg loss: 0.222749  [44700/89996]\n",
      "loss: 0.000139, rolling avg loss: 0.181179  [44800/89996]\n",
      "loss: 0.000701, rolling avg loss: 0.162241  [44900/89996]\n",
      "loss: 0.021892, rolling avg loss: 0.249215  [45000/89996]\n",
      "loss: 0.000000, rolling avg loss: 0.154003  [45100/89996]\n",
      "loss: 0.000001, rolling avg loss: 0.181027  [45200/89996]\n",
      "loss: 0.000070, rolling avg loss: 0.135724  [45300/89996]\n",
      "loss: 0.130131, rolling avg loss: 0.233315  [45400/89996]\n",
      "loss: 0.145530, rolling avg loss: 0.147707  [45500/89996]\n",
      "loss: 0.228116, rolling avg loss: 0.145056  [45600/89996]\n",
      "loss: 0.037285, rolling avg loss: 0.059655  [45700/89996]\n",
      "loss: 0.000121, rolling avg loss: 0.032723  [45800/89996]\n",
      "loss: 0.000000, rolling avg loss: 0.275948  [45900/89996]\n",
      "loss: 0.172822, rolling avg loss: 0.225194  [46000/89996]\n",
      "loss: 0.000003, rolling avg loss: 0.124021  [46100/89996]\n",
      "loss: 0.000000, rolling avg loss: 0.134139  [46200/89996]\n",
      "loss: 0.000060, rolling avg loss: 0.105462  [46300/89996]\n",
      "loss: 1.254474, rolling avg loss: 0.126331  [46400/89996]\n",
      "loss: 0.001757, rolling avg loss: 0.137700  [46500/89996]\n",
      "loss: 0.039144, rolling avg loss: 0.180542  [46600/89996]\n",
      "loss: 0.797472, rolling avg loss: 0.177558  [46700/89996]\n",
      "loss: 0.007840, rolling avg loss: 0.285211  [46800/89996]\n",
      "loss: 0.000000, rolling avg loss: 0.221486  [46900/89996]\n",
      "loss: 1.357544, rolling avg loss: 0.083929  [47000/89996]\n",
      "loss: 0.048777, rolling avg loss: 0.115781  [47100/89996]\n",
      "loss: 0.028049, rolling avg loss: 0.118939  [47200/89996]\n",
      "loss: 0.001846, rolling avg loss: 0.064911  [47300/89996]\n",
      "loss: 0.000003, rolling avg loss: 0.157976  [47400/89996]\n",
      "loss: 0.011831, rolling avg loss: 0.263373  [47500/89996]\n",
      "loss: 0.143420, rolling avg loss: 0.069124  [47600/89996]\n",
      "loss: 0.000000, rolling avg loss: 0.121540  [47700/89996]\n",
      "loss: 0.000715, rolling avg loss: 0.195627  [47800/89996]\n",
      "loss: 5.042870, rolling avg loss: 0.162326  [47900/89996]\n",
      "loss: 0.004197, rolling avg loss: 0.138383  [48000/89996]\n",
      "loss: 0.000154, rolling avg loss: 0.177245  [48100/89996]\n",
      "loss: 0.000003, rolling avg loss: 0.166157  [48200/89996]\n",
      "loss: 0.914426, rolling avg loss: 0.136278  [48300/89996]\n",
      "loss: 0.002331, rolling avg loss: 0.148344  [48400/89996]\n",
      "loss: 0.042635, rolling avg loss: 0.184436  [48500/89996]\n",
      "loss: 0.000101, rolling avg loss: 0.135743  [48600/89996]\n",
      "loss: 0.000230, rolling avg loss: 0.152742  [48700/89996]\n",
      "loss: 0.000010, rolling avg loss: 0.077191  [48800/89996]\n",
      "loss: 0.000021, rolling avg loss: 0.136320  [48900/89996]\n",
      "loss: 0.000511, rolling avg loss: 0.166946  [49000/89996]\n",
      "loss: 0.000277, rolling avg loss: 0.116836  [49100/89996]\n",
      "loss: 0.001815, rolling avg loss: 0.054196  [49200/89996]\n",
      "loss: 0.000451, rolling avg loss: 0.050495  [49300/89996]\n",
      "loss: 0.000001, rolling avg loss: 0.124468  [49400/89996]\n",
      "loss: 0.000004, rolling avg loss: 0.077186  [49500/89996]\n",
      "loss: 0.000000, rolling avg loss: 0.136546  [49600/89996]\n",
      "loss: 0.003957, rolling avg loss: 0.062509  [49700/89996]\n",
      "loss: 0.072860, rolling avg loss: 0.283646  [49800/89996]\n",
      "loss: 0.003201, rolling avg loss: 0.053056  [49900/89996]\n",
      "loss: 0.000150, rolling avg loss: 0.110865  [50000/89996]\n",
      "loss: 0.042191, rolling avg loss: 0.074542  [50100/89996]\n",
      "loss: 0.000000, rolling avg loss: 0.126794  [50200/89996]\n",
      "loss: 0.043841, rolling avg loss: 0.133434  [50300/89996]\n",
      "loss: 0.382357, rolling avg loss: 0.096235  [50400/89996]\n",
      "loss: 0.000000, rolling avg loss: 0.092284  [50500/89996]\n",
      "loss: 0.249571, rolling avg loss: 0.092895  [50600/89996]\n",
      "loss: 0.000718, rolling avg loss: 0.151354  [50700/89996]\n",
      "loss: 0.003125, rolling avg loss: 0.235911  [50800/89996]\n",
      "loss: 0.010097, rolling avg loss: 0.280860  [50900/89996]\n",
      "loss: 0.000046, rolling avg loss: 0.209415  [51000/89996]\n",
      "loss: 5.833848, rolling avg loss: 0.285350  [51100/89996]\n",
      "loss: 0.409259, rolling avg loss: 0.177281  [51200/89996]\n",
      "loss: 0.021333, rolling avg loss: 0.190201  [51300/89996]\n",
      "loss: 2.831397, rolling avg loss: 0.149379  [51400/89996]\n",
      "loss: 0.004037, rolling avg loss: 0.201806  [51500/89996]\n",
      "loss: 0.000055, rolling avg loss: 0.071282  [51600/89996]\n",
      "loss: 0.037484, rolling avg loss: 0.130481  [51700/89996]\n",
      "loss: 0.000368, rolling avg loss: 0.044611  [51800/89996]\n",
      "loss: 0.000099, rolling avg loss: 0.184838  [51900/89996]\n",
      "loss: 0.001011, rolling avg loss: 0.106534  [52000/89996]\n",
      "loss: 0.000170, rolling avg loss: 0.308829  [52100/89996]\n",
      "loss: 0.001518, rolling avg loss: 0.075128  [52200/89996]\n",
      "loss: 0.119205, rolling avg loss: 0.190698  [52300/89996]\n",
      "loss: 0.037228, rolling avg loss: 0.179954  [52400/89996]\n",
      "loss: 0.203686, rolling avg loss: 0.080433  [52500/89996]\n",
      "loss: 0.036549, rolling avg loss: 0.076758  [52600/89996]\n",
      "loss: 0.000000, rolling avg loss: 0.053515  [52700/89996]\n",
      "loss: 0.000093, rolling avg loss: 0.144951  [52800/89996]\n",
      "loss: 0.000000, rolling avg loss: 0.113517  [52900/89996]\n",
      "loss: 0.000000, rolling avg loss: 0.145671  [53000/89996]\n",
      "loss: 0.000107, rolling avg loss: 0.101755  [53100/89996]\n",
      "loss: 0.105205, rolling avg loss: 0.198302  [53200/89996]\n",
      "loss: 0.000414, rolling avg loss: 0.098631  [53300/89996]\n",
      "loss: 0.003107, rolling avg loss: 0.119038  [53400/89996]\n",
      "loss: 0.000372, rolling avg loss: 0.201260  [53500/89996]\n",
      "loss: 1.379363, rolling avg loss: 0.112830  [53600/89996]\n",
      "loss: 0.000019, rolling avg loss: 0.134357  [53700/89996]\n",
      "loss: 0.001198, rolling avg loss: 0.082216  [53800/89996]\n",
      "loss: 0.000009, rolling avg loss: 0.198586  [53900/89996]\n",
      "loss: 0.004033, rolling avg loss: 0.121458  [54000/89996]\n",
      "loss: 0.908433, rolling avg loss: 0.061528  [54100/89996]\n",
      "loss: 0.002930, rolling avg loss: 0.127273  [54200/89996]\n",
      "loss: 0.015719, rolling avg loss: 0.113099  [54300/89996]\n",
      "loss: 2.836818, rolling avg loss: 0.153137  [54400/89996]\n",
      "loss: 2.752824, rolling avg loss: 0.261397  [54500/89996]\n",
      "loss: 0.006211, rolling avg loss: 0.141497  [54600/89996]\n",
      "loss: 0.181555, rolling avg loss: 0.124850  [54700/89996]\n",
      "loss: 0.000000, rolling avg loss: 0.202404  [54800/89996]\n",
      "loss: 0.026980, rolling avg loss: 0.095296  [54900/89996]\n",
      "loss: 0.002917, rolling avg loss: 0.144645  [55000/89996]\n",
      "loss: 0.000014, rolling avg loss: 0.228607  [55100/89996]\n",
      "loss: 0.008140, rolling avg loss: 0.116120  [55200/89996]\n",
      "loss: 0.003625, rolling avg loss: 0.047280  [55300/89996]\n",
      "loss: 0.000142, rolling avg loss: 0.088659  [55400/89996]\n",
      "loss: 1.956958, rolling avg loss: 0.116077  [55500/89996]\n",
      "loss: 0.012572, rolling avg loss: 0.160648  [55600/89996]\n",
      "loss: 3.042819, rolling avg loss: 0.200957  [55700/89996]\n",
      "loss: 0.863346, rolling avg loss: 0.136831  [55800/89996]\n",
      "loss: 0.108951, rolling avg loss: 0.265043  [55900/89996]\n",
      "loss: 0.011583, rolling avg loss: 0.066165  [56000/89996]\n",
      "loss: 0.000648, rolling avg loss: 0.214476  [56100/89996]\n",
      "loss: 0.000008, rolling avg loss: 0.121858  [56200/89996]\n",
      "loss: 0.000106, rolling avg loss: 0.172403  [56300/89996]\n",
      "loss: 0.000000, rolling avg loss: 0.119438  [56400/89996]\n",
      "loss: 0.000000, rolling avg loss: 0.031228  [56500/89996]\n",
      "loss: 0.000567, rolling avg loss: 0.113412  [56600/89996]\n",
      "loss: 0.019408, rolling avg loss: 0.035216  [56700/89996]\n",
      "loss: 0.000001, rolling avg loss: 0.133226  [56800/89996]\n",
      "loss: 0.000633, rolling avg loss: 0.081256  [56900/89996]\n",
      "loss: 0.093186, rolling avg loss: 0.201344  [57000/89996]\n",
      "loss: 0.111511, rolling avg loss: 0.134220  [57100/89996]\n",
      "loss: 0.169684, rolling avg loss: 0.224944  [57200/89996]\n",
      "loss: 0.063179, rolling avg loss: 0.124838  [57300/89996]\n",
      "loss: 0.000023, rolling avg loss: 0.065381  [57400/89996]\n",
      "loss: 0.014739, rolling avg loss: 0.159417  [57500/89996]\n",
      "loss: 0.000000, rolling avg loss: 0.102193  [57600/89996]\n",
      "loss: 0.728147, rolling avg loss: 0.104418  [57700/89996]\n",
      "loss: 0.000141, rolling avg loss: 0.199506  [57800/89996]\n",
      "loss: 0.005292, rolling avg loss: 0.116315  [57900/89996]\n",
      "loss: 0.000009, rolling avg loss: 0.185876  [58000/89996]\n",
      "loss: 0.002035, rolling avg loss: 0.201448  [58100/89996]\n",
      "loss: 0.335743, rolling avg loss: 0.272366  [58200/89996]\n",
      "loss: 0.000000, rolling avg loss: 0.148986  [58300/89996]\n",
      "loss: 0.000000, rolling avg loss: 0.147620  [58400/89996]\n",
      "loss: 0.000000, rolling avg loss: 0.157211  [58500/89996]\n",
      "loss: 0.001457, rolling avg loss: 0.118533  [58600/89996]\n",
      "loss: 0.002167, rolling avg loss: 0.117206  [58700/89996]\n",
      "loss: 0.003874, rolling avg loss: 0.170488  [58800/89996]\n",
      "loss: 0.001687, rolling avg loss: 0.233553  [58900/89996]\n",
      "loss: 0.000002, rolling avg loss: 0.132442  [59000/89996]\n",
      "loss: 0.000000, rolling avg loss: 0.218585  [59100/89996]\n",
      "loss: 0.343412, rolling avg loss: 0.110600  [59200/89996]\n",
      "loss: 0.050873, rolling avg loss: 0.121259  [59300/89996]\n",
      "loss: 0.017060, rolling avg loss: 0.147932  [59400/89996]\n",
      "loss: 1.263749, rolling avg loss: 0.301707  [59500/89996]\n",
      "loss: 0.011691, rolling avg loss: 0.066559  [59600/89996]\n",
      "loss: 0.000806, rolling avg loss: 0.100343  [59700/89996]\n",
      "loss: 0.000001, rolling avg loss: 0.179587  [59800/89996]\n",
      "loss: 0.000023, rolling avg loss: 0.140438  [59900/89996]\n",
      "loss: 0.000007, rolling avg loss: 0.101603  [60000/89996]\n",
      "loss: 0.000120, rolling avg loss: 0.139136  [60100/89996]\n",
      "loss: 0.000177, rolling avg loss: 0.068837  [60200/89996]\n",
      "loss: 0.002677, rolling avg loss: 0.152550  [60300/89996]\n",
      "loss: 0.001847, rolling avg loss: 0.075121  [60400/89996]\n",
      "loss: 0.000402, rolling avg loss: 0.195091  [60500/89996]\n",
      "loss: 0.000121, rolling avg loss: 0.025475  [60600/89996]\n",
      "loss: 0.000012, rolling avg loss: 0.131143  [60700/89996]\n",
      "loss: 0.023064, rolling avg loss: 0.057852  [60800/89996]\n",
      "loss: 0.007259, rolling avg loss: 0.207437  [60900/89996]\n",
      "loss: 0.000017, rolling avg loss: 0.176679  [61000/89996]\n",
      "loss: 2.134669, rolling avg loss: 0.123996  [61100/89996]\n",
      "loss: 0.020443, rolling avg loss: 0.117849  [61200/89996]\n",
      "loss: 0.000000, rolling avg loss: 0.216961  [61300/89996]\n",
      "loss: 0.467417, rolling avg loss: 0.271435  [61400/89996]\n",
      "loss: 0.000074, rolling avg loss: 0.050463  [61500/89996]\n",
      "loss: 0.002943, rolling avg loss: 0.096535  [61600/89996]\n",
      "loss: 0.000000, rolling avg loss: 0.111035  [61700/89996]\n",
      "loss: 0.000652, rolling avg loss: 0.111369  [61800/89996]\n",
      "loss: 0.091000, rolling avg loss: 0.098588  [61900/89996]\n",
      "loss: 0.003391, rolling avg loss: 0.065558  [62000/89996]\n",
      "loss: 0.000078, rolling avg loss: 0.292170  [62100/89996]\n",
      "loss: 0.003315, rolling avg loss: 0.183851  [62200/89996]\n",
      "loss: 0.259075, rolling avg loss: 0.145884  [62300/89996]\n",
      "loss: 0.000406, rolling avg loss: 0.186979  [62400/89996]\n",
      "loss: 0.018864, rolling avg loss: 0.153886  [62500/89996]\n",
      "loss: 0.435297, rolling avg loss: 0.171871  [62600/89996]\n",
      "loss: 0.000185, rolling avg loss: 0.059816  [62700/89996]\n",
      "loss: 0.001181, rolling avg loss: 0.152535  [62800/89996]\n",
      "loss: 0.005930, rolling avg loss: 0.042326  [62900/89996]\n",
      "loss: 0.000000, rolling avg loss: 0.083546  [63000/89996]\n",
      "loss: 0.004655, rolling avg loss: 0.244070  [63100/89996]\n",
      "loss: 2.418623, rolling avg loss: 0.206285  [63200/89996]\n",
      "loss: 0.678302, rolling avg loss: 0.218364  [63300/89996]\n",
      "loss: 0.135582, rolling avg loss: 0.292735  [63400/89996]\n",
      "loss: 0.001896, rolling avg loss: 0.122943  [63500/89996]\n",
      "loss: 0.000003, rolling avg loss: 0.114524  [63600/89996]\n",
      "loss: 0.000000, rolling avg loss: 0.119620  [63700/89996]\n",
      "loss: 0.077473, rolling avg loss: 0.172071  [63800/89996]\n",
      "loss: 0.532982, rolling avg loss: 0.189641  [63900/89996]\n",
      "loss: 0.001620, rolling avg loss: 0.040531  [64000/89996]\n",
      "loss: 0.009701, rolling avg loss: 0.160400  [64100/89996]\n",
      "loss: 0.000021, rolling avg loss: 0.136479  [64200/89996]\n",
      "loss: 0.000026, rolling avg loss: 0.093920  [64300/89996]\n",
      "loss: 0.000866, rolling avg loss: 0.166416  [64400/89996]\n",
      "loss: 0.001876, rolling avg loss: 0.218609  [64500/89996]\n",
      "loss: 0.002478, rolling avg loss: 0.071913  [64600/89996]\n",
      "loss: 0.000896, rolling avg loss: 0.103064  [64700/89996]\n",
      "loss: 0.015173, rolling avg loss: 0.060577  [64800/89996]\n",
      "loss: 0.000018, rolling avg loss: 0.020960  [64900/89996]\n",
      "loss: 0.583799, rolling avg loss: 0.244202  [65000/89996]\n",
      "loss: 0.000001, rolling avg loss: 0.120056  [65100/89996]\n",
      "loss: 0.000000, rolling avg loss: 0.067773  [65200/89996]\n",
      "loss: 0.000001, rolling avg loss: 0.058041  [65300/89996]\n",
      "loss: 0.000000, rolling avg loss: 0.040915  [65400/89996]\n",
      "loss: 0.026320, rolling avg loss: 0.181079  [65500/89996]\n",
      "loss: 0.080938, rolling avg loss: 0.201734  [65600/89996]\n",
      "loss: 0.555115, rolling avg loss: 0.117763  [65700/89996]\n",
      "loss: 0.000013, rolling avg loss: 0.114834  [65800/89996]\n",
      "loss: 0.000202, rolling avg loss: 0.213566  [65900/89996]\n",
      "loss: 0.918727, rolling avg loss: 0.098392  [66000/89996]\n",
      "loss: 0.000000, rolling avg loss: 0.141340  [66100/89996]\n",
      "loss: 0.001060, rolling avg loss: 0.165708  [66200/89996]\n",
      "loss: 0.000006, rolling avg loss: 0.111532  [66300/89996]\n",
      "loss: 0.054851, rolling avg loss: 0.136166  [66400/89996]\n",
      "loss: 0.000341, rolling avg loss: 0.075517  [66500/89996]\n",
      "loss: 0.002228, rolling avg loss: 0.157236  [66600/89996]\n",
      "loss: 0.031765, rolling avg loss: 0.180629  [66700/89996]\n",
      "loss: 0.028604, rolling avg loss: 0.122884  [66800/89996]\n",
      "loss: 0.044782, rolling avg loss: 0.080309  [66900/89996]\n",
      "loss: 0.013234, rolling avg loss: 0.046288  [67000/89996]\n",
      "loss: 0.000292, rolling avg loss: 0.076395  [67100/89996]\n",
      "loss: 0.000000, rolling avg loss: 0.242441  [67200/89996]\n",
      "loss: 0.019723, rolling avg loss: 0.156424  [67300/89996]\n",
      "loss: 0.000000, rolling avg loss: 0.039292  [67400/89996]\n",
      "loss: 0.000003, rolling avg loss: 0.194828  [67500/89996]\n",
      "loss: 0.641668, rolling avg loss: 0.264850  [67600/89996]\n",
      "loss: 0.383687, rolling avg loss: 0.075700  [67700/89996]\n",
      "loss: 0.213870, rolling avg loss: 0.243401  [67800/89996]\n",
      "loss: 0.001840, rolling avg loss: 0.065213  [67900/89996]\n",
      "loss: 0.003452, rolling avg loss: 0.112054  [68000/89996]\n",
      "loss: 0.111065, rolling avg loss: 0.108631  [68100/89996]\n",
      "loss: 0.000000, rolling avg loss: 0.113550  [68200/89996]\n",
      "loss: 0.000000, rolling avg loss: 0.120768  [68300/89996]\n",
      "loss: 0.000334, rolling avg loss: 0.059854  [68400/89996]\n",
      "loss: 0.000007, rolling avg loss: 0.136866  [68500/89996]\n",
      "loss: 0.002962, rolling avg loss: 0.059419  [68600/89996]\n",
      "loss: 0.000003, rolling avg loss: 0.096743  [68700/89996]\n",
      "loss: 0.000647, rolling avg loss: 0.113158  [68800/89996]\n",
      "loss: 0.003858, rolling avg loss: 0.169235  [68900/89996]\n",
      "loss: 0.072271, rolling avg loss: 0.048140  [69000/89996]\n",
      "loss: 0.000000, rolling avg loss: 0.172070  [69100/89996]\n",
      "loss: 0.263815, rolling avg loss: 0.143517  [69200/89996]\n",
      "loss: 0.124951, rolling avg loss: 0.130352  [69300/89996]\n",
      "loss: 0.000000, rolling avg loss: 0.129373  [69400/89996]\n",
      "loss: 0.000000, rolling avg loss: 0.071093  [69500/89996]\n",
      "loss: 0.000000, rolling avg loss: 0.206244  [69600/89996]\n",
      "loss: 0.000000, rolling avg loss: 0.039277  [69700/89996]\n",
      "loss: 0.067983, rolling avg loss: 0.043334  [69800/89996]\n",
      "loss: 0.000000, rolling avg loss: 0.118472  [69900/89996]\n",
      "loss: 0.000002, rolling avg loss: 0.202526  [70000/89996]\n",
      "loss: 0.000824, rolling avg loss: 0.100202  [70100/89996]\n",
      "loss: 0.000089, rolling avg loss: 0.139508  [70200/89996]\n",
      "loss: 0.035518, rolling avg loss: 0.119622  [70300/89996]\n",
      "loss: 0.043594, rolling avg loss: 0.048229  [70400/89996]\n",
      "loss: 0.000439, rolling avg loss: 0.020517  [70500/89996]\n",
      "loss: 0.000001, rolling avg loss: 0.084575  [70600/89996]\n",
      "loss: 0.023345, rolling avg loss: 0.075080  [70700/89996]\n",
      "loss: 0.000000, rolling avg loss: 0.117425  [70800/89996]\n",
      "loss: 0.000065, rolling avg loss: 0.101498  [70900/89996]\n",
      "loss: 0.000000, rolling avg loss: 0.193371  [71000/89996]\n",
      "loss: 0.001493, rolling avg loss: 0.188743  [71100/89996]\n",
      "loss: 0.838036, rolling avg loss: 0.127791  [71200/89996]\n",
      "loss: 0.012409, rolling avg loss: 0.063945  [71300/89996]\n",
      "loss: 6.580091, rolling avg loss: 0.162739  [71400/89996]\n",
      "loss: 0.006553, rolling avg loss: 0.088135  [71500/89996]\n",
      "loss: 0.000001, rolling avg loss: 0.192178  [71600/89996]\n",
      "loss: 0.000000, rolling avg loss: 0.088014  [71700/89996]\n",
      "loss: 0.000000, rolling avg loss: 0.126709  [71800/89996]\n",
      "loss: 0.000045, rolling avg loss: 0.082274  [71900/89996]\n",
      "loss: 0.000004, rolling avg loss: 0.087812  [72000/89996]\n",
      "loss: 0.000288, rolling avg loss: 0.078366  [72100/89996]\n",
      "loss: 0.000031, rolling avg loss: 0.050280  [72200/89996]\n",
      "loss: 0.000695, rolling avg loss: 0.123233  [72300/89996]\n",
      "loss: 0.005369, rolling avg loss: 0.119622  [72400/89996]\n",
      "loss: 0.190762, rolling avg loss: 0.133688  [72500/89996]\n",
      "loss: 0.000008, rolling avg loss: 0.127334  [72600/89996]\n",
      "loss: 0.000000, rolling avg loss: 0.153876  [72700/89996]\n",
      "loss: 0.004997, rolling avg loss: 0.206441  [72800/89996]\n",
      "loss: 0.000000, rolling avg loss: 0.091249  [72900/89996]\n",
      "loss: 0.000025, rolling avg loss: 0.124483  [73000/89996]\n",
      "loss: 0.000009, rolling avg loss: 0.067444  [73100/89996]\n",
      "loss: 0.000003, rolling avg loss: 0.211931  [73200/89996]\n",
      "loss: 0.001946, rolling avg loss: 0.168304  [73300/89996]\n",
      "loss: 0.000290, rolling avg loss: 0.095747  [73400/89996]\n",
      "loss: 0.000000, rolling avg loss: 0.058478  [73500/89996]\n",
      "loss: 0.000000, rolling avg loss: 0.127274  [73600/89996]\n",
      "loss: 0.000643, rolling avg loss: 0.126975  [73700/89996]\n",
      "loss: 0.000061, rolling avg loss: 0.140513  [73800/89996]\n",
      "loss: 0.000001, rolling avg loss: 0.121689  [73900/89996]\n",
      "loss: 0.001708, rolling avg loss: 0.122414  [74000/89996]\n",
      "loss: 0.000033, rolling avg loss: 0.116053  [74100/89996]\n",
      "loss: 0.015170, rolling avg loss: 0.084777  [74200/89996]\n",
      "loss: 0.000000, rolling avg loss: 0.166601  [74300/89996]\n",
      "loss: 0.000042, rolling avg loss: 0.253261  [74400/89996]\n",
      "loss: 0.000000, rolling avg loss: 0.300893  [74500/89996]\n",
      "loss: 0.000196, rolling avg loss: 0.085741  [74600/89996]\n",
      "loss: 0.000069, rolling avg loss: 0.116846  [74700/89996]\n",
      "loss: 0.000093, rolling avg loss: 0.126224  [74800/89996]\n",
      "loss: 0.000005, rolling avg loss: 0.117460  [74900/89996]\n",
      "loss: 0.000487, rolling avg loss: 0.107985  [75000/89996]\n",
      "loss: 0.000032, rolling avg loss: 0.091352  [75100/89996]\n",
      "loss: 0.005968, rolling avg loss: 0.211378  [75200/89996]\n",
      "loss: 0.589252, rolling avg loss: 0.127741  [75300/89996]\n",
      "loss: 0.000000, rolling avg loss: 0.166360  [75400/89996]\n",
      "loss: 0.001031, rolling avg loss: 0.065013  [75500/89996]\n",
      "loss: 0.000071, rolling avg loss: 0.055583  [75600/89996]\n",
      "loss: 0.002275, rolling avg loss: 0.150669  [75700/89996]\n",
      "loss: 0.000000, rolling avg loss: 0.068537  [75800/89996]\n",
      "loss: 0.000991, rolling avg loss: 0.129278  [75900/89996]\n",
      "loss: 0.000009, rolling avg loss: 0.199910  [76000/89996]\n",
      "loss: 0.000000, rolling avg loss: 0.121710  [76100/89996]\n",
      "loss: 0.000001, rolling avg loss: 0.118792  [76200/89996]\n",
      "loss: 0.382933, rolling avg loss: 0.176921  [76300/89996]\n",
      "loss: 0.000000, rolling avg loss: 0.224282  [76400/89996]\n",
      "loss: 0.191764, rolling avg loss: 0.110445  [76500/89996]\n",
      "loss: 0.000455, rolling avg loss: 0.072678  [76600/89996]\n",
      "loss: 0.012361, rolling avg loss: 0.227650  [76700/89996]\n",
      "loss: 0.001042, rolling avg loss: 0.169538  [76800/89996]\n",
      "loss: 0.118933, rolling avg loss: 0.111983  [76900/89996]\n",
      "loss: 0.000023, rolling avg loss: 0.134510  [77000/89996]\n",
      "loss: 0.000323, rolling avg loss: 0.101111  [77100/89996]\n",
      "loss: 0.000008, rolling avg loss: 0.104716  [77200/89996]\n",
      "loss: 0.000986, rolling avg loss: 0.083419  [77300/89996]\n",
      "loss: 0.000016, rolling avg loss: 0.197572  [77400/89996]\n",
      "loss: 0.000149, rolling avg loss: 0.246474  [77500/89996]\n",
      "loss: 0.000001, rolling avg loss: 0.123151  [77600/89996]\n",
      "loss: 0.000000, rolling avg loss: 0.106029  [77700/89996]\n",
      "loss: 0.283064, rolling avg loss: 0.131778  [77800/89996]\n",
      "loss: 0.001048, rolling avg loss: 0.046567  [77900/89996]\n",
      "loss: 0.023117, rolling avg loss: 0.079657  [78000/89996]\n",
      "loss: 0.000455, rolling avg loss: 0.070193  [78100/89996]\n",
      "loss: 0.000000, rolling avg loss: 0.184604  [78200/89996]\n",
      "loss: 0.034994, rolling avg loss: 0.086417  [78300/89996]\n",
      "loss: 0.000002, rolling avg loss: 0.101091  [78400/89996]\n",
      "loss: 0.054175, rolling avg loss: 0.099920  [78500/89996]\n",
      "loss: 0.009739, rolling avg loss: 0.084763  [78600/89996]\n",
      "loss: 0.471389, rolling avg loss: 0.129570  [78700/89996]\n",
      "loss: 0.020480, rolling avg loss: 0.126363  [78800/89996]\n",
      "loss: 0.014571, rolling avg loss: 0.071761  [78900/89996]\n",
      "loss: 0.000003, rolling avg loss: 0.173051  [79000/89996]\n",
      "loss: 0.028615, rolling avg loss: 0.105685  [79100/89996]\n",
      "loss: 0.000278, rolling avg loss: 0.193219  [79200/89996]\n",
      "loss: 0.182926, rolling avg loss: 0.087217  [79300/89996]\n",
      "loss: 0.000006, rolling avg loss: 0.210706  [79400/89996]\n",
      "loss: 0.000044, rolling avg loss: 0.443137  [79500/89996]\n",
      "loss: 0.000002, rolling avg loss: 0.069991  [79600/89996]\n",
      "loss: 0.002163, rolling avg loss: 0.108168  [79700/89996]\n",
      "loss: 0.000000, rolling avg loss: 0.122751  [79800/89996]\n",
      "loss: 0.000431, rolling avg loss: 0.074769  [79900/89996]\n",
      "loss: 0.000134, rolling avg loss: 0.300868  [80000/89996]\n",
      "loss: 0.006697, rolling avg loss: 0.118400  [80100/89996]\n",
      "loss: 0.000024, rolling avg loss: 0.271341  [80200/89996]\n",
      "loss: 0.000035, rolling avg loss: 0.048111  [80300/89996]\n",
      "loss: 0.000043, rolling avg loss: 0.195739  [80400/89996]\n",
      "loss: 0.003167, rolling avg loss: 0.122592  [80500/89996]\n",
      "loss: 0.021889, rolling avg loss: 0.199944  [80600/89996]\n",
      "loss: 0.000002, rolling avg loss: 0.132682  [80700/89996]\n",
      "loss: 0.477365, rolling avg loss: 0.098176  [80800/89996]\n",
      "loss: 0.000492, rolling avg loss: 0.014396  [80900/89996]\n",
      "loss: 0.000000, rolling avg loss: 0.230711  [81000/89996]\n",
      "loss: 0.079758, rolling avg loss: 0.200184  [81100/89996]\n",
      "loss: 0.000006, rolling avg loss: 0.104081  [81200/89996]\n",
      "loss: 0.000751, rolling avg loss: 0.219689  [81300/89996]\n",
      "loss: 0.189512, rolling avg loss: 0.082994  [81400/89996]\n",
      "loss: 0.000000, rolling avg loss: 0.107843  [81500/89996]\n",
      "loss: 0.000000, rolling avg loss: 0.086349  [81600/89996]\n",
      "loss: 0.021664, rolling avg loss: 0.209880  [81700/89996]\n",
      "loss: 0.000025, rolling avg loss: 0.076050  [81800/89996]\n",
      "loss: 0.000000, rolling avg loss: 0.052592  [81900/89996]\n",
      "loss: 0.000003, rolling avg loss: 0.078041  [82000/89996]\n",
      "loss: 0.000009, rolling avg loss: 0.033279  [82100/89996]\n",
      "loss: 0.000002, rolling avg loss: 0.018835  [82200/89996]\n",
      "loss: 0.000068, rolling avg loss: 0.089132  [82300/89996]\n",
      "loss: 0.000013, rolling avg loss: 0.139354  [82400/89996]\n",
      "loss: 0.000000, rolling avg loss: 0.154523  [82500/89996]\n",
      "loss: 0.000000, rolling avg loss: 0.104727  [82600/89996]\n",
      "loss: 0.163856, rolling avg loss: 0.093571  [82700/89996]\n",
      "loss: 0.069092, rolling avg loss: 0.198965  [82800/89996]\n",
      "loss: 0.004040, rolling avg loss: 0.269845  [82900/89996]\n",
      "loss: 0.002136, rolling avg loss: 0.076600  [83000/89996]\n",
      "loss: 0.529136, rolling avg loss: 0.097031  [83100/89996]\n",
      "loss: 0.000027, rolling avg loss: 0.040890  [83200/89996]\n",
      "loss: 0.000006, rolling avg loss: 0.040191  [83300/89996]\n",
      "loss: 0.099976, rolling avg loss: 0.075816  [83400/89996]\n",
      "loss: 0.002712, rolling avg loss: 0.034389  [83500/89996]\n",
      "loss: 0.000443, rolling avg loss: 0.173281  [83600/89996]\n",
      "loss: 0.000143, rolling avg loss: 0.061200  [83700/89996]\n",
      "loss: 0.000149, rolling avg loss: 0.095832  [83800/89996]\n",
      "loss: 0.000000, rolling avg loss: 0.213148  [83900/89996]\n",
      "loss: 0.000000, rolling avg loss: 0.055067  [84000/89996]\n",
      "loss: 0.000018, rolling avg loss: 0.036842  [84100/89996]\n",
      "loss: 0.000295, rolling avg loss: 0.162605  [84200/89996]\n",
      "loss: 0.000184, rolling avg loss: 0.254601  [84300/89996]\n",
      "loss: 0.020135, rolling avg loss: 0.173530  [84400/89996]\n",
      "loss: 0.000000, rolling avg loss: 0.107673  [84500/89996]\n",
      "loss: 0.744513, rolling avg loss: 0.207820  [84600/89996]\n",
      "loss: 0.302453, rolling avg loss: 0.065316  [84700/89996]\n",
      "loss: 0.000241, rolling avg loss: 0.039599  [84800/89996]\n",
      "loss: 0.000001, rolling avg loss: 0.088927  [84900/89996]\n",
      "loss: 0.329657, rolling avg loss: 0.269914  [85000/89996]\n",
      "loss: 0.000000, rolling avg loss: 0.115567  [85100/89996]\n",
      "loss: 0.219517, rolling avg loss: 0.102701  [85200/89996]\n",
      "loss: 0.000056, rolling avg loss: 0.255385  [85300/89996]\n",
      "loss: 0.000011, rolling avg loss: 0.132226  [85400/89996]\n",
      "loss: 0.207212, rolling avg loss: 0.144172  [85500/89996]\n",
      "loss: 0.000180, rolling avg loss: 0.102728  [85600/89996]\n",
      "loss: 0.265873, rolling avg loss: 0.153611  [85700/89996]\n",
      "loss: 0.000716, rolling avg loss: 0.098893  [85800/89996]\n",
      "loss: 0.032122, rolling avg loss: 0.116242  [85900/89996]\n",
      "loss: 0.015528, rolling avg loss: 0.129653  [86000/89996]\n",
      "loss: 0.001634, rolling avg loss: 0.164389  [86100/89996]\n",
      "loss: 0.000012, rolling avg loss: 0.156720  [86200/89996]\n",
      "loss: 0.008770, rolling avg loss: 0.072913  [86300/89996]\n",
      "loss: 0.051060, rolling avg loss: 0.101874  [86400/89996]\n",
      "loss: 0.000162, rolling avg loss: 0.041140  [86500/89996]\n",
      "loss: 0.000005, rolling avg loss: 0.058396  [86600/89996]\n",
      "loss: 0.000194, rolling avg loss: 0.075843  [86700/89996]\n",
      "loss: 0.075689, rolling avg loss: 0.186265  [86800/89996]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    train_loop(train_dataloader, classifier, criterion, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4f7b01a3-3df2-43b9-819a-fce6e063f74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_loop(dataloader, model, loss_fn, num_classes=9):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss = 0\n",
    "    \n",
    "    # Initialize confusion matrix\n",
    "    confusion_matrix = torch.zeros(num_classes, num_classes)\n",
    "    \n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            \n",
    "            # Get predicted class\n",
    "            predicted = pred.argmax(1)\n",
    "            \n",
    "            # Update confusion matrix\n",
    "            for t, p in zip(y.view(-1), predicted.view(-1)):\n",
    "                confusion_matrix[t.long(), p.long()] += 1\n",
    "    \n",
    "    # Calculate average loss\n",
    "    test_loss /= num_batches\n",
    "    \n",
    "    # Calculate metrics\n",
    "    metrics = {}\n",
    "    \n",
    "    # Overall accuracy\n",
    "    accuracy = confusion_matrix.diag().sum() / confusion_matrix.sum()\n",
    "    \n",
    "    # Per-class metrics\n",
    "    for class_idx in range(num_classes):\n",
    "        # True positives: diagonal elements\n",
    "        tp = confusion_matrix[class_idx, class_idx]\n",
    "        \n",
    "        # False positives: sum of column minus true positive\n",
    "        fp = confusion_matrix[:, class_idx].sum() - tp\n",
    "        \n",
    "        # False negatives: sum of row minus true positive\n",
    "        fn = confusion_matrix[class_idx, :].sum() - tp\n",
    "        \n",
    "        # True negatives: all minus tp, fp, fn\n",
    "        tn = confusion_matrix.sum() - tp - fp - fn\n",
    "        \n",
    "        # Calculate metrics\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        \n",
    "        metrics[f\"class_{class_idx}\"] = {\n",
    "            \"precision\": precision.item(),\n",
    "            \"recall\": recall.item(),\n",
    "            \"f1\": f1.item(),\n",
    "            \"true_positives\": tp.item(),\n",
    "            \"false_positives\": fp.item(),\n",
    "            \"true_negatives\": tn.item(),\n",
    "            \"false_negatives\": fn.item()\n",
    "        }\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"Test Results: \\n Accuracy: {(100*accuracy):>0.1f}%, Avg loss: {test_loss:>8f}\")\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(confusion_matrix)\n",
    "    print(\"\\nPer-class metrics:\")\n",
    "    for class_idx, class_metrics in metrics.items():\n",
    "        print(f\"{class_idx}:\")\n",
    "        print(f\"  Precision: {class_metrics['precision']:.4f}\")\n",
    "        print(f\"  Recall: {class_metrics['recall']:.4f}\")\n",
    "        print(f\"  F1-Score: {class_metrics['f1']:.4f}\")\n",
    "        print(f\"  TP: {class_metrics['true_positives']}, FP: {class_metrics['false_positives']}\")\n",
    "        print(f\"  TN: {class_metrics['true_negatives']}, FN: {class_metrics['false_negatives']}\")\n",
    "        print(\"\")\n",
    "    \n",
    "    return {\n",
    "        \"loss\": test_loss,\n",
    "        \"accuracy\": accuracy.item(),\n",
    "        \"confusion_matrix\": confusion_matrix,\n",
    "        \"class_metrics\": metrics\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c4d6c213-62c6-4b55-b1e6-7aaa447f63db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results: \n",
      " Accuracy: 91.3%, Avg loss: 0.314782\n",
      "\n",
      "Confusion Matrix:\n",
      "tensor([[1.2960e+03, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 3.8000e+01,\n",
      "         0.0000e+00, 0.0000e+00, 4.0000e+00],\n",
      "        [6.0000e+00, 8.4000e+02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 3.2000e+02, 3.0000e+00, 0.0000e+00, 1.4000e+01,\n",
      "         0.0000e+00, 0.0000e+00, 2.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 1.0000e+00, 6.3100e+02, 0.0000e+00, 1.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 1.0000e+00],\n",
      "        [4.0000e+00, 0.0000e+00, 6.0000e+00, 0.0000e+00, 9.6900e+02, 2.3000e+01,\n",
      "         1.6000e+01, 1.2000e+01, 5.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 2.0000e+00, 0.0000e+00, 0.0000e+00, 5.5800e+02,\n",
      "         0.0000e+00, 3.2000e+01, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 6.0000e+00, 1.0000e+00, 3.0000e+00,\n",
      "         6.8900e+02, 0.0000e+00, 4.2000e+01],\n",
      "        [0.0000e+00, 0.0000e+00, 5.0000e+00, 0.0000e+00, 3.0000e+00, 2.7900e+02,\n",
      "         0.0000e+00, 1.3000e+02, 4.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 5.4000e+01, 9.0000e+00, 2.0000e+00, 6.0000e+00,\n",
      "         3.9000e+01, 3.0000e+00, 1.1200e+03]])\n",
      "\n",
      "Per-class metrics:\n",
      "class_0:\n",
      "  Precision: 0.9923\n",
      "  Recall: 0.9686\n",
      "  F1-Score: 0.9803\n",
      "  TP: 1296.0, FP: 10.0\n",
      "  TN: 5832.0, FN: 42.0\n",
      "\n",
      "class_1:\n",
      "  Precision: 1.0000\n",
      "  Recall: 0.9917\n",
      "  F1-Score: 0.9959\n",
      "  TP: 840.0, FP: 0.0\n",
      "  TN: 6333.0, FN: 7.0\n",
      "\n",
      "class_2:\n",
      "  Precision: 0.8247\n",
      "  Recall: 0.9440\n",
      "  F1-Score: 0.8803\n",
      "  TP: 320.0, FP: 68.0\n",
      "  TN: 6773.0, FN: 19.0\n",
      "\n",
      "class_3:\n",
      "  Precision: 0.9723\n",
      "  Recall: 0.9953\n",
      "  F1-Score: 0.9836\n",
      "  TP: 631.0, FP: 18.0\n",
      "  TN: 6528.0, FN: 3.0\n",
      "\n",
      "class_4:\n",
      "  Precision: 0.9938\n",
      "  Recall: 0.9362\n",
      "  F1-Score: 0.9642\n",
      "  TP: 969.0, FP: 6.0\n",
      "  TN: 6139.0, FN: 66.0\n",
      "\n",
      "class_5:\n",
      "  Precision: 0.6046\n",
      "  Recall: 0.9426\n",
      "  F1-Score: 0.7366\n",
      "  TP: 558.0, FP: 365.0\n",
      "  TN: 6223.0, FN: 34.0\n",
      "\n",
      "class_6:\n",
      "  Precision: 0.9261\n",
      "  Recall: 0.9298\n",
      "  F1-Score: 0.9279\n",
      "  TP: 689.0, FP: 55.0\n",
      "  TN: 6384.0, FN: 52.0\n",
      "\n",
      "class_7:\n",
      "  Precision: 0.7345\n",
      "  Recall: 0.3088\n",
      "  F1-Score: 0.4348\n",
      "  TP: 130.0, FP: 47.0\n",
      "  TN: 6712.0, FN: 291.0\n",
      "\n",
      "class_8:\n",
      "  Precision: 0.9508\n",
      "  Recall: 0.9084\n",
      "  F1-Score: 0.9291\n",
      "  TP: 1120.0, FP: 58.0\n",
      "  TN: 5889.0, FN: 113.0\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'loss': 0.3147820494856667,\n",
       " 'accuracy': 0.912674069404602,\n",
       " 'confusion_matrix': tensor([[1.2960e+03, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 3.8000e+01,\n",
       "          0.0000e+00, 0.0000e+00, 4.0000e+00],\n",
       "         [6.0000e+00, 8.4000e+02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "         [0.0000e+00, 0.0000e+00, 3.2000e+02, 3.0000e+00, 0.0000e+00, 1.4000e+01,\n",
       "          0.0000e+00, 0.0000e+00, 2.0000e+00],\n",
       "         [0.0000e+00, 0.0000e+00, 1.0000e+00, 6.3100e+02, 0.0000e+00, 1.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 1.0000e+00],\n",
       "         [4.0000e+00, 0.0000e+00, 6.0000e+00, 0.0000e+00, 9.6900e+02, 2.3000e+01,\n",
       "          1.6000e+01, 1.2000e+01, 5.0000e+00],\n",
       "         [0.0000e+00, 0.0000e+00, 2.0000e+00, 0.0000e+00, 0.0000e+00, 5.5800e+02,\n",
       "          0.0000e+00, 3.2000e+01, 0.0000e+00],\n",
       "         [0.0000e+00, 0.0000e+00, 0.0000e+00, 6.0000e+00, 1.0000e+00, 3.0000e+00,\n",
       "          6.8900e+02, 0.0000e+00, 4.2000e+01],\n",
       "         [0.0000e+00, 0.0000e+00, 5.0000e+00, 0.0000e+00, 3.0000e+00, 2.7900e+02,\n",
       "          0.0000e+00, 1.3000e+02, 4.0000e+00],\n",
       "         [0.0000e+00, 0.0000e+00, 5.4000e+01, 9.0000e+00, 2.0000e+00, 6.0000e+00,\n",
       "          3.9000e+01, 3.0000e+00, 1.1200e+03]]),\n",
       " 'class_metrics': {'class_0': {'precision': 0.992343008518219,\n",
       "   'recall': 0.9686098694801331,\n",
       "   'f1': 0.9803327918052673,\n",
       "   'true_positives': 1296.0,\n",
       "   'false_positives': 10.0,\n",
       "   'true_negatives': 5832.0,\n",
       "   'false_negatives': 42.0},\n",
       "  'class_1': {'precision': 1.0,\n",
       "   'recall': 0.9917355179786682,\n",
       "   'f1': 0.9958506226539612,\n",
       "   'true_positives': 840.0,\n",
       "   'false_positives': 0.0,\n",
       "   'true_negatives': 6333.0,\n",
       "   'false_negatives': 7.0},\n",
       "  'class_2': {'precision': 0.8247422575950623,\n",
       "   'recall': 0.9439527988433838,\n",
       "   'f1': 0.8803300857543945,\n",
       "   'true_positives': 320.0,\n",
       "   'false_positives': 68.0,\n",
       "   'true_negatives': 6773.0,\n",
       "   'false_negatives': 19.0},\n",
       "  'class_3': {'precision': 0.9722650051116943,\n",
       "   'recall': 0.9952681660652161,\n",
       "   'f1': 0.9836321473121643,\n",
       "   'true_positives': 631.0,\n",
       "   'false_positives': 18.0,\n",
       "   'true_negatives': 6528.0,\n",
       "   'false_negatives': 3.0},\n",
       "  'class_4': {'precision': 0.9938461780548096,\n",
       "   'recall': 0.9362319111824036,\n",
       "   'f1': 0.9641791582107544,\n",
       "   'true_positives': 969.0,\n",
       "   'false_positives': 6.0,\n",
       "   'true_negatives': 6139.0,\n",
       "   'false_negatives': 66.0},\n",
       "  'class_5': {'precision': 0.6045503616333008,\n",
       "   'recall': 0.9425675868988037,\n",
       "   'f1': 0.7366336584091187,\n",
       "   'true_positives': 558.0,\n",
       "   'false_positives': 365.0,\n",
       "   'true_negatives': 6223.0,\n",
       "   'false_negatives': 34.0},\n",
       "  'class_6': {'precision': 0.926075279712677,\n",
       "   'recall': 0.9298245906829834,\n",
       "   'f1': 0.927946150302887,\n",
       "   'true_positives': 689.0,\n",
       "   'false_positives': 55.0,\n",
       "   'true_negatives': 6384.0,\n",
       "   'false_negatives': 52.0},\n",
       "  'class_7': {'precision': 0.7344632744789124,\n",
       "   'recall': 0.30878859758377075,\n",
       "   'f1': 0.43478259444236755,\n",
       "   'true_positives': 130.0,\n",
       "   'false_positives': 47.0,\n",
       "   'true_negatives': 6712.0,\n",
       "   'false_negatives': 291.0},\n",
       "  'class_8': {'precision': 0.950764000415802,\n",
       "   'recall': 0.9083536267280579,\n",
       "   'f1': 0.9290750622749329,\n",
       "   'true_positives': 1120.0,\n",
       "   'false_positives': 58.0,\n",
       "   'true_negatives': 5889.0,\n",
       "   'false_negatives': 113.0}}}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_loop(test_dataloader, classifier, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538a57e9-992e-439a-8129-7fe97f69a891",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(confusion_matrix, class_names=None, normalize=False, title=\"Confusion Matrix\", cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    Plot a confusion matrix with clear labels and colorbar.\n",
    "    \n",
    "    Args:\n",
    "        confusion_matrix: The confusion matrix from the test_loop function\n",
    "        class_names: List of class names (optional, defaults to indices)\n",
    "        normalize: Boolean to normalize values (default: False)\n",
    "        title: Plot title (default: \"Confusion Matrix\")\n",
    "        cmap: Colormap (default: plt.cm.Blues)\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    # Convert to numpy array if it's a torch tensor\n",
    "    if hasattr(confusion_matrix, 'cpu'):\n",
    "        cm = confusion_matrix.cpu().numpy()\n",
    "    else:\n",
    "        cm = np.array(confusion_matrix)\n",
    "    \n",
    "    # Set up class names\n",
    "    if class_names is None:\n",
    "        class_names = [str(i) for i in range(cm.shape[0])]\n",
    "    \n",
    "    # Normalize if requested\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        fmt = '.2f'\n",
    "    else:\n",
    "        fmt = 'd'\n",
    "    \n",
    "    # Create figure and axis\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title, fontsize=15)\n",
    "    plt.colorbar()\n",
    "    \n",
    "    # Set up tick marks\n",
    "    tick_marks = np.arange(len(class_names))\n",
    "    plt.xticks(tick_marks, class_names, rotation=45, ha='right', fontsize=10)\n",
    "    plt.yticks(tick_marks, class_names, fontsize=10)\n",
    "    \n",
    "    # Add text annotations to each cell\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            plt.text(j, i, format(cm[i, j], fmt),\n",
    "                     ha=\"center\", va=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\",\n",
    "                     fontsize=9)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label', fontsize=12)\n",
    "    plt.xlabel('Predicted label', fontsize=12)\n",
    "    \n",
    "    return plt\n",
    "\n",
    "# Example usage within the test_loop function or after it returns:\n",
    "def test_with_visualization(dataloader, model, loss_fn, num_classes=9):\n",
    "    # Call the test_loop function\n",
    "    results = test_loop(dataloader, model, loss_fn, num_classes)\n",
    "    \n",
    "    # Get the confusion matrix from results\n",
    "    conf_matrix = results['confusion_matrix']\n",
    "    \n",
    "    # Optional: Define class names (replace with your actual class names)\n",
    "    class_names = [label_names[value] for value in label_names]\n",
    "    \n",
    "    # Plot and display the confusion matrix\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    \n",
    "    # Plot raw counts\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plot_confusion_matrix(\n",
    "        conf_matrix, \n",
    "        class_names=class_names,\n",
    "        title=\"Confusion Matrix (Counts)\"\n",
    "    )\n",
    "    \n",
    "    # Plot normalized (percentage)\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plot_confusion_matrix(\n",
    "        conf_matrix, \n",
    "        class_names=class_names,\n",
    "        normalize=True,\n",
    "        title=\"Confusion Matrix (Normalized)\"\n",
    "    )\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a5c689-e7e3-48ae-a1bc-60b028e0fe7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier(torch.unsqueeze(dataset[2][0], dim=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04590ee1-b4a2-4f38-9bd0-d1dd399f5c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[2][1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
